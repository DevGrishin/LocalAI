# LocalAI

LocalAI is a C++/Qt 6 application that provides a ChatGPT-style interface to interact with local AI models using [Ollama](https://ollama.ai). It leverages QML for the UI and supports modern local LLMs with minimal overhead.

---

## üöÄ Features

- Local AI chat interface
- Modern QML-based GUI
- Integration with Ollama for local model inference

---

## üß∞ Prerequisites

### ‚úÖ Qt 6.5 or newer

Install Qt with the following modules:

- `Qt Quick`
- `Qt Sql`

You can install via [Qt Online Installer](https://www.qt.io/download) or through your package manager.

### ‚úÖ CMake 3.16 or newer

Install CMake:

- On Linux: `sudo apt install cmake`
- On macOS: `brew install cmake`
- On Windows: via [cmake.org](https://cmake.org/download/)

### ‚úÖ Compiler

You‚Äôll need a C++17-compatible compiler (GCC, Clang, or MSVC).

---

## üß† Install Ollama

LocalAI requires [Ollama](https://ollama.ai) to run local language models.

1. Download and install Ollama from [https://ollama.ai](https://ollama.ai)
2. Start Ollama (it typically runs as a background service)
3. Pull a model:
   ```bash
   ollama pull llama2
   ```
   or a smaller version:
   ```bash
   ollama pull llama2:7b
   ```

---

## üîß Building the Project

### Clone the repository

```bash
git clone https://github.com/your-username/LocalAI.git
cd LocalAI
```

### Configure and build

```bash
mkdir build
cd build
cmake ..
cmake --build .
```

> üí° If Qt isn‚Äôt found automatically, you may need to set `CMAKE_PREFIX_PATH`:
> ```bash
> export CMAKE_PREFIX_PATH=/path/to/Qt/6.5.0/gcc_64/lib/cmake
> ```

---

## ‚ñ∂Ô∏è Running the App

From the `build/` directory:

```bash
./appLocalAI
```

(On Windows: `appLocalAI.exe`)

---

## üì¶ Installation (Optional)

To install the application system-wide (e.g., on Linux):

```bash
cmake --install .
```

